name: Backend load test (Redis + queue)

on:
  push:
    paths:
      - "backend/**"
      - ".github/workflows/backend-load-test.yml"
  pull_request:
    paths:
      - "backend/**"
      - ".github/workflows/backend-load-test.yml"

jobs:
  load-test:
    runs-on: ubuntu-latest
    env:
      SYNQC_API_KEY: ci-test-key
      SYNQC_REDIS_URL: redis://localhost:6379/0
      SYNQC_WORKER_POOL_SIZE: 6
      SYNQC_ENABLE_METRICS: "true"
      ALERTMANAGER_CONFIG_TEMPLATE: ${{ vars.ALERTMANAGER_CONFIG_TEMPLATE || 'backend/ops/alertmanager-ci.yml' }}
      ALERTMANAGER_CONFIG_RENDERED: /tmp/alertmanager.yml
      LOAD_TEST_RUNS: ${{ vars.LOAD_TEST_RUNS || 60 }}
      LOAD_TEST_CONCURRENCY: ${{ vars.LOAD_TEST_CONCURRENCY || 15 }}
      SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      SLACK_CHANNEL: ${{ secrets.SLACK_CHANNEL }}
      PAGERDUTY_ROUTING_KEY: ${{ secrets.PAGERDUTY_ROUTING_KEY }}
      WEBHOOK_MIRROR_URL: ${{ secrets.WEBHOOK_MIRROR_URL }}
    services:
      redis:
        image: redis:7
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 5s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install backend dependencies
        working-directory: backend
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]

      - name: Ensure staging alert secrets are present when required
        if: env.ALERTMANAGER_CONFIG_TEMPLATE == 'backend/ops/alertmanager-staging.yml'
        env:
          SLACK_WEBHOOK_URL: ${{ env.SLACK_WEBHOOK_URL }}
          SLACK_CHANNEL: ${{ env.SLACK_CHANNEL }}
          PAGERDUTY_ROUTING_KEY: ${{ env.PAGERDUTY_ROUTING_KEY }}
          WEBHOOK_MIRROR_URL: ${{ env.WEBHOOK_MIRROR_URL }}
        run: |
          missing=()
          [ -n "${SLACK_WEBHOOK_URL:-}" ] || missing+=(SLACK_WEBHOOK_URL)
          [ -n "${SLACK_CHANNEL:-}" ] || missing+=(SLACK_CHANNEL)
          [ -n "${PAGERDUTY_ROUTING_KEY:-}" ] || missing+=(PAGERDUTY_ROUTING_KEY)
          [ -n "${WEBHOOK_MIRROR_URL:-}" ] || missing+=(WEBHOOK_MIRROR_URL)
          if [ ${#missing[@]} -gt 0 ]; then
            echo "Staging Alertmanager template requires secrets: ${missing[*]}"
            exit 1
          fi

      - name: Render Alertmanager config (CI or staging)
        env:
          ALERTMANAGER_CONFIG_TEMPLATE: ${{ env.ALERTMANAGER_CONFIG_TEMPLATE }}
          ALERTMANAGER_CONFIG_RENDERED: ${{ env.ALERTMANAGER_CONFIG_RENDERED }}
        run: |
          envsubst < "${ALERTMANAGER_CONFIG_TEMPLATE}" > "${ALERTMANAGER_CONFIG_RENDERED}"
          echo "Rendered Alertmanager config to ${ALERTMANAGER_CONFIG_RENDERED}"

      - name: Start backend (uvicorn + metrics)
        working-directory: backend
        env:
          SYNQC_API_KEY: ${{ env.SYNQC_API_KEY }}
          SYNQC_REDIS_URL: ${{ env.SYNQC_REDIS_URL }}
          SYNQC_WORKER_POOL_SIZE: ${{ env.SYNQC_WORKER_POOL_SIZE }}
          SYNQC_ENABLE_METRICS: ${{ env.SYNQC_ENABLE_METRICS }}
        run: |
          uvicorn synqc_backend.api:app --host 0.0.0.0 --port 8001 > /tmp/uvicorn.log 2>&1 &
          echo $! > /tmp/uvicorn.pid
          sleep 5
          curl -f http://127.0.0.1:8001/health

      - name: Start alert webhook sink
        run: |
          python - <<'PY' > /tmp/alert-webhook.log 2>&1 &
          import json
          import http.server

          class Handler(http.server.BaseHTTPRequestHandler):
              def do_POST(self):
                  length = int(self.headers.get("Content-Length", 0))
                  body = self.rfile.read(length)
                  with open("/tmp/alertmanager-webhook.log", "ab") as log:
                      log.write(body + b"\n")
                  self.send_response(200)
                  self.end_headers()

          http.server.ThreadingHTTPServer(("0.0.0.0", 9094), Handler).serve_forever()
          PY
          echo $! > /tmp/alert-webhook.pid

      - name: Start Alertmanager
        run: |
          docker run -d --name synqc-alertmanager \
            -v "${{ env.ALERTMANAGER_CONFIG_RENDERED }}:/etc/alertmanager/alertmanager.yml" \
            --add-host host.docker.internal:host-gateway \
            -p 9093:9093 prom/alertmanager:latest >/tmp/alertmanager.cid
          sleep 5
          curl -f http://127.0.0.1:9093/api/v2/status

      - name: Start Prometheus scrape of CI backend
        working-directory: backend/ops
        run: |
          docker run -d --name synqc-prometheus \
            -v "${{ github.workspace }}/backend/ops/prometheus-ci-scrape.yml:/etc/prometheus/prometheus.yml" \
            -v "${{ github.workspace }}/backend/ops/prometheus-synqc-example.yml:/etc/prometheus/synqc-alerts.yml" \
            --add-host host.docker.internal:host-gateway \
            -p 9090:9090 prom/prometheus:latest >/tmp/prometheus.cid
          sleep 10
          curl -f "http://127.0.0.1:9090/api/v1/targets" | python - <<'PY'
          import json, sys
          data = json.load(sys.stdin)
          active = data.get("data", {}).get("activeTargets", [])
          if not any(t.get("health") == "up" for t in active):
              raise SystemExit("Prometheus scrape target not healthy")
          PY

      - name: Run multi-worker queue/budget load test
        working-directory: backend
        env:
          SYNQC_API_KEY: ${{ env.SYNQC_API_KEY }}
        run: |
          python scripts/load_test.py \
            --base-url http://127.0.0.1:8001 \
            --metrics-url http://127.0.0.1:9000/metrics \
            --runs "${LOAD_TEST_RUNS}" --concurrency "${LOAD_TEST_CONCURRENCY}" \
            --api-key "${SYNQC_API_KEY}" \
            --session-id ci-load-test \
            --timeout 240 \
            --strict

      - name: Verify Prometheus alerts did not fire
        run: |
          # Assert Alertmanager saw no alerts during a healthy run
          alerts=$(curl -s http://127.0.0.1:9090/api/v1/alerts)
          echo "Prometheus alerts: $alerts"
          if [ -s /tmp/alertmanager-webhook.log ]; then
            echo "Unexpected alerts routed to webhook:"
            cat /tmp/alertmanager-webhook.log
            exit 1
          fi

      - name: Dump backend logs on failure
        if: failure()
        run: |
          echo "=== uvicorn.log ==="
          if [ -f /tmp/uvicorn.log ]; then cat /tmp/uvicorn.log; fi
          if [ -f /tmp/uvicorn.pid ]; then ps -p $(cat /tmp/uvicorn.pid); fi

      - name: Stop backend
        if: always()
        run: |
          if [ -f /tmp/uvicorn.pid ]; then kill $(cat /tmp/uvicorn.pid) || true; fi
          if docker ps -a --format '{{.Names}}' | grep -q synqc-prometheus; then docker rm -f synqc-prometheus || true; fi
          if docker ps -a --format '{{.Names}}' | grep -q synqc-alertmanager; then docker rm -f synqc-alertmanager || true; fi
          if [ -f /tmp/alert-webhook.pid ]; then kill $(cat /tmp/alert-webhook.pid) || true; fi
